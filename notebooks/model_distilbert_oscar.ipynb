{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/oscar/.cache/pypoetry/virtualenvs/zrive-ds-TXjvcAVs-py3.11/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import logging\n",
    "import torch\n",
    "from datasets import Dataset, DatasetDict\n",
    "from transformers import DistilBertModel\n",
    "from transformers import AutoTokenizer, DistilBertTokenizer\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from datasets import Dataset, DatasetDict\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(levelname)s: %(message)s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/oscar/.cache/pypoetry/virtualenvs/zrive-ds-TXjvcAVs-py3.11/lib/python3.11/site-packages/torch/cuda/__init__.py:141: UserWarning: CUDA initialization: The NVIDIA driver on your system is too old (found version 11060). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver. (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:108.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    }
   ],
   "source": [
    "from torch import cuda\n",
    "device = 'cuda' if cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: NAICS codes processed successfully. Here's the head of the processed DataFrame:\n",
      "INFO: \n",
      "   NAICS                               BUSINESS_DESCRIPTION  label\n",
      "0     72  Zenyai Viet Cajun & Pho Restaurant is dedicate...      0\n",
      "1     54  Kilduff Underground Engineering, Inc. (KUE) is...      1\n",
      "2     45  024™ is a premium home fragrance brand that de...      2\n",
      "3     56  Our Services include Office Cleaning Carpet cl...      3\n",
      "4     62                    NYS Licensed Home Health Agency      4\n"
     ]
    }
   ],
   "source": [
    "data_path = '../../data/coverwallet/coverwallet.xlsx'\n",
    "df = pd.read_excel(data_path)\n",
    "df = df.dropna()\n",
    "def truncate_naics_and_prepare_data(df, column_name, num_digits):\n",
    "    \"\"\"\n",
    "    Truncates the NAICS codes in the specified column to the desired number of digits.\n",
    "\n",
    "    :param df: pandas DataFrame containing the NAICS codes.\n",
    "    :param column_name: the name of the column with the NAICS codes.\n",
    "    :param num_digits: the number of digits to truncate to.\n",
    "    :return: A copy of the DataFrame with the NAICS codes truncated.\n",
    "    \"\"\"\n",
    "    # Validate the number of digits\n",
    "    if not isinstance(num_digits, int) or num_digits <= 0:\n",
    "        logging.error(\"Number of digits must be a positive integer\")\n",
    "        raise ValueError(\"Number of digits must be a positive integer\")\n",
    "    \n",
    "    # Make a copy of the DataFrame to avoid modifying the original\n",
    "    df_copy = df.copy()\n",
    "    def truncate_code(code):\n",
    "        \"\"\"\n",
    "        Truncates or pads the NAICS code to the specified number of digits.\n",
    "        :param code: the NAICS code to be truncated.\n",
    "        :return: The truncated or original NAICS code as a string.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Ensure the code is a string\n",
    "            code_str = str(code)\n",
    "            # Truncate the code if it's longer than num_digits\n",
    "            return code_str[:num_digits].ljust(num_digits, '0')\n",
    "        except Exception as e:\n",
    "            logging.exception(\"Error truncating code: {}\".format(code))\n",
    "            return code\n",
    "        \n",
    "    # Apply the truncation function to the specified column\n",
    "    df_copy[column_name] = df_copy[column_name].apply(truncate_code)\n",
    "    # Try to convert the truncated column to integers\n",
    "    try:\n",
    "        df_copy[column_name] = df_copy[column_name].astype(int)\n",
    "    except ValueError as e:\n",
    "        logging.warning(\"Could not convert truncated codes to integers: {}\".format(e))\n",
    "        # Keep the column as strings if conversion fails\n",
    "        pass\n",
    "    \n",
    "    labels = df_copy['NAICS'].unique().tolist()\n",
    "    id2label = {idx: label for idx, label in enumerate(labels)}\n",
    "    label2id = {label: idx for idx, label in enumerate(labels)}\n",
    "    df_copy['label'] = df_copy['NAICS'].map(label2id)\n",
    "    logging.info(\"NAICS codes processed successfully. Here's the head of the processed DataFrame:\")\n",
    "    logging.info(\"\\n%s\", df_copy.head())\n",
    "    df_copy_train, df_copy_final_val = train_test_split(df_copy, test_size=0.15, shuffle=True, random_state=42)\n",
    "    \n",
    "    dataset_train = Dataset.from_pandas(df_copy_train)\n",
    "    dataset_final_val = Dataset.from_pandas(df_copy_final_val)\n",
    "    \n",
    "    return df_copy,  dataset_train, dataset_final_val\n",
    "'''\n",
    "# Configuration k-fold\n",
    "    num_folds = 3\n",
    "    kf = KFold(n_splits=num_folds, shuffle=True, random_state=42)\n",
    "    kfold_datasets = []\n",
    "\n",
    "    for fold, (train_indices, val_indices) in enumerate(kf.split(dataset_train)):\n",
    "        train_dataset = dataset_train.select(train_indices)\n",
    "        val_dataset = dataset_train.select(val_indices)\n",
    "        \n",
    "        dataset_dict = {\n",
    "            'train': train_dataset,\n",
    "            'validation': val_dataset\n",
    "        }\n",
    "\n",
    "        features_dict = {\n",
    "            \"NAICS\": dataset_train[\"NAICS\"],\n",
    "            \"BUSINESS_DESCRIPTION\": dataset_train[\"BUSINESS_DESCRIPTION\"],\n",
    "        }\n",
    "    \n",
    "        kfold_datasets.append(dataset_dict)\n",
    "        logging.info(f\"Processed fold {fold + 1}\")\n",
    "\n",
    "    for i, dataset_dict in enumerate(kfold_datasets):\n",
    "        for split in dataset_dict.keys():\n",
    "            dataset_dict[split] = dataset_dict[split].map(lambda example: {key: example[key] for key in features_dict.keys()})\n",
    "\n",
    "        logging.info(f\"DatasetDict for Fold {i + 1}:\")\n",
    "        for split, dataset in dataset_dict.items():\n",
    "            logging.info(f\"  {split} split: {dataset}\")\n",
    "            \n",
    "    logging.info(\"NAICS codes truncated successfully. Here's the head of the truncated DataFrame:\")\n",
    "    logging.info(\"\\n%s\", df_copy.head())\n",
    "    logging.info(\"Number of unique NAICS labels: %d\", len(labels))\n",
    "    '''\n",
    "    #return df_copy, kfold_datasets, dataset_train, dataset_final_val\n",
    "\n",
    "    #df_2_digits, kfold_2_digits, dataset_train_2_digits, dataset_final_val_2_digits = truncate_naics_and_prepare_data(df, 'NAICS', 2)\n",
    "df_2_digits, dataset_train_2_digits, dataset_final_val_2_digits = truncate_naics_and_prepare_data(df, 'NAICS', 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**HYPERPARAMETERS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['NAICS', 'BUSINESS_DESCRIPTION', 'label'], dtype='object')\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23]\n"
     ]
    }
   ],
   "source": [
    "# Defining some key variables that will be used later on in the training\n",
    "MAX_LEN = 128\n",
    "TRAIN_BATCH_SIZE = 4\n",
    "VALID_BATCH_SIZE = 4\n",
    "EPOCHS = 3\n",
    "LEARNING_RATE = 1e-05\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased', truncation=True, do_lower_case=True)\n",
    "print(df_2_digits.columns)\n",
    "print(sorted(df_2_digits['label'].unique()))\n",
    "df_2_digits['BUSINESS_DESCRIPTION'] = df_2_digits['BUSINESS_DESCRIPTION'].apply(lambda x: ' '.join(x) if isinstance(x, np.ndarray) else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = 0.8\n",
    "test_size = 0.1 \n",
    "\n",
    "train_data, test_data = train_test_split(df_2_digits, test_size=test_size, random_state=200)\n",
    "\n",
    "remaining_size = 1.0 - train_size - test_size\n",
    "if remaining_size > 0:\n",
    "    val_size = remaining_size\n",
    "    # Dividir el resto en conjunto de validación\n",
    "    train_data, val_data = train_test_split(train_data, test_size=val_size, random_state=200)\n",
    "else:\n",
    "    val_data = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN Dataset: (11481, 3)\n",
      "VALIDATION Dataset: (1276, 3)\n",
      "TEST Dataset: (1418, 3)\n"
     ]
    }
   ],
   "source": [
    "print(\"TRAIN Dataset: {}\".format(train_data.shape))\n",
    "print(\"VALIDATION Dataset: {}\".format(val_data.shape))\n",
    "print(\"TEST Dataset: {}\".format(test_data.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loaders generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer, max_len):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.data = dataframe\n",
    "        self.texts = np.array(dataframe['BUSINESS_DESCRIPTION'].astype(str))\n",
    "        self.targets = np.array(dataframe['label'])  \n",
    "        self.max_len = max_len\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        texts = (self.texts[idx])\n",
    "        target = self.targets[idx]\n",
    "        inputs_ids= []\n",
    "        attention_masks= []\n",
    "        targets= []\n",
    "        for text in texts:  # Iterar sobre cada texto en el array.\n",
    "            inputs = self.tokenizer.encode_plus(\n",
    "                text,\n",
    "                add_special_tokens=True,\n",
    "                max_length=self.max_len,\n",
    "                padding='max_length',\n",
    "                truncation=True,\n",
    "                return_attention_mask=True,\n",
    "                return_tensors='pt',\n",
    "            )\n",
    "            input_ids = inputs['input_ids'].squeeze()  # Squeeze para remover dimensiones extras\n",
    "            attention_mask = inputs['attention_mask'].squeeze()\n",
    "            targets= torch.tensor(target, dtype=torch.long).squeeze()\n",
    "            inputs_ids.append(input_ids)\n",
    "            attention_masks.append(attention_mask)\n",
    "            #targets.append(target)\n",
    "\n",
    "        return {\n",
    "            \n",
    "            'input_ids': inputs_ids,\n",
    "            'attention_mask': attention_masks,\n",
    "            'labels': targets\n",
    "        }\n",
    "\n",
    "    @property\n",
    "    def data(self):\n",
    "        return self._data\n",
    "\n",
    "    @data.setter\n",
    "    def data(self, new_data):\n",
    "        \"\"\"\n",
    "        Setter para el atributo 'data'. Este método permite cambiar el valor de 'data'.\n",
    "        \"\"\"\n",
    "        self._data = new_data\n",
    "        # Actualizar 'text' y 'targets' si es necesario\n",
    "        #self.text = new_data['BUSINESS_DESCRIPTION']\n",
    "        #self.targets = list(new_data['label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Creación de los loaders**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased', truncation=True, do_lower_case=True)\n",
    "\n",
    "training_set = CustomDataset(train_data, tokenizer, MAX_LEN)\n",
    "val_set = CustomDataset(val_data, tokenizer, MAX_LEN)\n",
    "\n",
    "training_loader = DataLoader(dataset=training_set, batch_size=TRAIN_BATCH_SIZE, shuffle=True, num_workers=0)\n",
    "val_loader = DataLoader(dataset=val_set, batch_size=VALID_BATCH_SIZE, shuffle=True, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next(iter(training_loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comprobación de que funcionan los loaders**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(val_loader))\n",
    "print(dir(val_loader))\n",
    "print(\"Cantidad de lotes en training_loader:\", len(val_loader))\n",
    "\n",
    "for i, batch in enumerate(training_loader):\n",
    "    \n",
    "    print(f\"Input ids: {len(batch['input_ids'])}\")\n",
    "    print(f\"Attention mask: {len(batch['attention_mask'])}\")\n",
    "    print(f\"labels: {batch['labels']}\")\n",
    "\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DistilBERTClass(\n",
       "  (l1): DistilBertModel(\n",
       "    (embeddings): Embeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (transformer): Transformer(\n",
       "      (layer): ModuleList(\n",
       "        (0-5): 6 x TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=24, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class DistilBERTClass(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DistilBERTClass, self).__init__()\n",
    "        self.l1 = DistilBertModel.from_pretrained(\"distilbert-base-uncased\")\n",
    "        self.pre_classifier = torch.nn.Linear(768, 768)\n",
    "        self.dropout = torch.nn.Dropout(0.1)\n",
    "        self.classifier = torch.nn.Linear(768, 24)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        output_1 = self.l1(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        hidden_state = output_1[0] # [4,128,768]\n",
    "        pooler = hidden_state[:, 0] # [4,768]\n",
    "        pooler = self.pre_classifier(pooler) # [4,768]\n",
    "        pooler = torch.nn.Tanh()(pooler)\n",
    "        pooler = self.dropout(pooler) # [4,768]\n",
    "        output = self.classifier(pooler) # [4,24]\n",
    "        return output\n",
    "    \n",
    "model = DistilBERTClass()\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(outputs, targets):\n",
    "    return torch.nn.CrossEntropyLoss()(outputs, targets)\n",
    "\n",
    "optimizer = torch.optim.Adam(params =  model.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total trainable parameters in DistilBERTClass: 66971928\n"
     ]
    }
   ],
   "source": [
    "total_params_distilbert = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Total trainable parameters in DistilBERTClass: {total_params_distilbert}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcuate_accu(big_idx, targets):\n",
    "    n_correct = (big_idx==targets).sum().item()\n",
    "    return n_correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    tr_loss = 0\n",
    "    n_correct = 0\n",
    "    nb_tr_steps = 0\n",
    "    nb_tr_examples = 0\n",
    "    model.train()\n",
    "    for _,data in enumerate(training_loader, 0):\n",
    "        ids = data['input_ids'].to(device, dtype = torch.long)\n",
    "        mask = data['attention_mask'].to(device, dtype = torch.long)\n",
    "        targets = data['labels'].to(device, dtype = torch.long).squeeze(1)\n",
    "\n",
    "        outputs = model(ids, mask)\n",
    "        loss = loss_fn(outputs, targets)\n",
    "        tr_loss += loss.item()\n",
    "        big_val, big_idx = torch.max(outputs.data, dim=1)\n",
    "        n_correct += calcuate_accu(big_idx, targets)\n",
    "\n",
    "        nb_tr_steps += 1\n",
    "        nb_tr_examples+=targets.size(0)\n",
    "\n",
    "        if _%5000==0:\n",
    "            loss_step = tr_loss/nb_tr_steps\n",
    "            accu_step = (n_correct*100)/nb_tr_examples\n",
    "            print(f\"Training Loss per 5000 steps: {loss_step}\")\n",
    "            print(f\"Training Accuracy per 5000 steps: {accu_step}\")\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        # # When using GPU\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f'The Total Accuracy for Epoch {epoch}: {(n_correct*100)/nb_tr_examples}')\n",
    "    epoch_loss = tr_loss/nb_tr_steps\n",
    "    epoch_accu = (n_correct*100)/nb_tr_examples\n",
    "    print(f\"Training Loss Epoch: {epoch_loss}\")\n",
    "    print(f\"Training Accuracy Epoch: {epoch_accu}\")\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss per 5000 steps: 3.2072134017944336\n",
      "Training Accuracy per 5000 steps: 0.0\n",
      "Training Loss per 5000 steps: 3.1389453411102295\n",
      "Training Accuracy per 5000 steps: 8.333333333333334\n",
      "Training Loss per 5000 steps: 3.108534661206332\n",
      "Training Accuracy per 5000 steps: 13.636363636363637\n",
      "Training Loss per 5000 steps: 3.072743609547615\n",
      "Training Accuracy per 5000 steps: 14.0625\n",
      "Training Loss per 5000 steps: 3.068045445850917\n",
      "Training Accuracy per 5000 steps: 16.666666666666668\n",
      "Training Loss per 5000 steps: 3.036367838199322\n",
      "Training Accuracy per 5000 steps: 17.307692307692307\n",
      "Training Loss per 5000 steps: 2.9885304820153022\n",
      "Training Accuracy per 5000 steps: 17.741935483870968\n",
      "Training Loss per 5000 steps: 2.984246075153351\n",
      "Training Accuracy per 5000 steps: 18.055555555555557\n",
      "Training Loss per 5000 steps: 2.9414890452129083\n",
      "Training Accuracy per 5000 steps: 20.73170731707317\n",
      "Training Loss per 5000 steps: 2.9098126110823257\n",
      "Training Accuracy per 5000 steps: 22.282608695652176\n",
      "Training Loss per 5000 steps: 2.872433858759263\n",
      "Training Accuracy per 5000 steps: 21.568627450980394\n",
      "Training Loss per 5000 steps: 2.825268362249647\n",
      "Training Accuracy per 5000 steps: 23.214285714285715\n",
      "Training Loss per 5000 steps: 2.774771891656469\n",
      "Training Accuracy per 5000 steps: 26.229508196721312\n",
      "Training Loss per 5000 steps: 2.754695056062756\n",
      "Training Accuracy per 5000 steps: 26.515151515151516\n",
      "Training Loss per 5000 steps: 2.721690854556124\n",
      "Training Accuracy per 5000 steps: 27.464788732394368\n",
      "Training Loss per 5000 steps: 2.714314564278251\n",
      "Training Accuracy per 5000 steps: 27.30263157894737\n",
      "Training Loss per 5000 steps: 2.6843572001398344\n",
      "Training Accuracy per 5000 steps: 28.703703703703702\n",
      "Training Loss per 5000 steps: 2.6612249945485313\n",
      "Training Accuracy per 5000 steps: 29.651162790697676\n",
      "Training Loss per 5000 steps: 2.6724832424750695\n",
      "Training Accuracy per 5000 steps: 29.12087912087912\n",
      "Training Loss per 5000 steps: 2.6467204727232456\n",
      "Training Accuracy per 5000 steps: 29.6875\n",
      "Training Loss per 5000 steps: 2.6245415671036976\n",
      "Training Accuracy per 5000 steps: 30.198019801980198\n",
      "Training Loss per 5000 steps: 2.5823308926708295\n",
      "Training Accuracy per 5000 steps: 31.60377358490566\n",
      "Training Loss per 5000 steps: 2.5552692273715594\n",
      "Training Accuracy per 5000 steps: 32.65765765765766\n",
      "Training Loss per 5000 steps: 2.554926143638019\n",
      "Training Accuracy per 5000 steps: 32.974137931034484\n",
      "Training Loss per 5000 steps: 2.5256058539240813\n",
      "Training Accuracy per 5000 steps: 34.09090909090909\n",
      "Training Loss per 5000 steps: 2.5033521803598555\n",
      "Training Accuracy per 5000 steps: 34.72222222222222\n",
      "Training Loss per 5000 steps: 2.505318175745374\n",
      "Training Accuracy per 5000 steps: 34.35114503816794\n",
      "Training Loss per 5000 steps: 2.492847774835194\n",
      "Training Accuracy per 5000 steps: 34.55882352941177\n",
      "Training Loss per 5000 steps: 2.477228519764352\n",
      "Training Accuracy per 5000 steps: 34.9290780141844\n",
      "Training Loss per 5000 steps: 2.4747525412742406\n",
      "Training Accuracy per 5000 steps: 34.76027397260274\n",
      "Training Loss per 5000 steps: 2.4771112377280433\n",
      "Training Accuracy per 5000 steps: 34.602649006622514\n",
      "Training Loss per 5000 steps: 2.4616943995157876\n",
      "Training Accuracy per 5000 steps: 34.93589743589744\n",
      "Training Loss per 5000 steps: 2.443185557860025\n",
      "Training Accuracy per 5000 steps: 35.559006211180126\n",
      "Training Loss per 5000 steps: 2.4302541933145867\n",
      "Training Accuracy per 5000 steps: 35.993975903614455\n",
      "Training Loss per 5000 steps: 2.412425261143355\n",
      "Training Accuracy per 5000 steps: 36.40350877192982\n",
      "Training Loss per 5000 steps: 2.408516510982405\n",
      "Training Accuracy per 5000 steps: 36.36363636363637\n",
      "Training Loss per 5000 steps: 2.388554252642953\n",
      "Training Accuracy per 5000 steps: 36.87845303867403\n",
      "Training Loss per 5000 steps: 2.374250550103444\n",
      "Training Accuracy per 5000 steps: 37.365591397849464\n",
      "Training Loss per 5000 steps: 2.36535168661497\n",
      "Training Accuracy per 5000 steps: 37.56544502617801\n",
      "Training Loss per 5000 steps: 2.3514792861378924\n",
      "Training Accuracy per 5000 steps: 37.755102040816325\n",
      "Training Loss per 5000 steps: 2.345634697978176\n",
      "Training Accuracy per 5000 steps: 37.81094527363184\n",
      "Training Loss per 5000 steps: 2.3211067229220013\n",
      "Training Accuracy per 5000 steps: 38.592233009708735\n",
      "Training Loss per 5000 steps: 2.2997803238895833\n",
      "Training Accuracy per 5000 steps: 39.45497630331754\n",
      "Training Loss per 5000 steps: 2.291788848185981\n",
      "Training Accuracy per 5000 steps: 39.467592592592595\n",
      "Training Loss per 5000 steps: 2.2649972409144787\n",
      "Training Accuracy per 5000 steps: 40.38461538461539\n",
      "Training Loss per 5000 steps: 2.258917792708473\n",
      "Training Accuracy per 5000 steps: 40.376106194690266\n",
      "Training Loss per 5000 steps: 2.239338381078852\n",
      "Training Accuracy per 5000 steps: 40.90909090909091\n",
      "Training Loss per 5000 steps: 2.2346591273859397\n",
      "Training Accuracy per 5000 steps: 40.889830508474574\n",
      "Training Loss per 5000 steps: 2.2334525922769335\n",
      "Training Accuracy per 5000 steps: 40.66390041493776\n",
      "Training Loss per 5000 steps: 2.231857142918478\n",
      "Training Accuracy per 5000 steps: 40.65040650406504\n",
      "Training Loss per 5000 steps: 2.228296723142563\n",
      "Training Accuracy per 5000 steps: 40.63745019920319\n",
      "Training Loss per 5000 steps: 2.222765745711513\n",
      "Training Accuracy per 5000 steps: 40.625\n",
      "Training Loss per 5000 steps: 2.211897892399309\n",
      "Training Accuracy per 5000 steps: 40.804597701149426\n",
      "Training Loss per 5000 steps: 2.2032907366528547\n",
      "Training Accuracy per 5000 steps: 41.16541353383459\n",
      "Training Loss per 5000 steps: 2.198324822301794\n",
      "Training Accuracy per 5000 steps: 41.32841328413284\n",
      "Training Loss per 5000 steps: 2.184288914544859\n",
      "Training Accuracy per 5000 steps: 41.757246376811594\n",
      "Training Loss per 5000 steps: 2.171010710060384\n",
      "Training Accuracy per 5000 steps: 42.170818505338076\n",
      "Training Loss per 5000 steps: 2.1702943418201035\n",
      "Training Accuracy per 5000 steps: 42.13286713286713\n",
      "Training Loss per 5000 steps: 2.168556699126037\n",
      "Training Accuracy per 5000 steps: 42.2680412371134\n",
      "Training Loss per 5000 steps: 2.165301769267063\n",
      "Training Accuracy per 5000 steps: 42.398648648648646\n",
      "Training Loss per 5000 steps: 2.1495608373535826\n",
      "Training Accuracy per 5000 steps: 42.857142857142854\n",
      "Training Loss per 5000 steps: 2.145002272101789\n",
      "Training Accuracy per 5000 steps: 42.97385620915033\n",
      "Training Loss per 5000 steps: 2.135315001298377\n",
      "Training Accuracy per 5000 steps: 43.167202572347264\n",
      "Training Loss per 5000 steps: 2.1282835433943363\n",
      "Training Accuracy per 5000 steps: 43.19620253164557\n",
      "Training Loss per 5000 steps: 2.1181921586626413\n",
      "Training Accuracy per 5000 steps: 43.45794392523364\n",
      "Training Loss per 5000 steps: 2.111879170215203\n",
      "Training Accuracy per 5000 steps: 43.71165644171779\n",
      "Training Loss per 5000 steps: 2.105611078598708\n",
      "Training Accuracy per 5000 steps: 43.957703927492446\n",
      "Training Loss per 5000 steps: 2.1013332281616472\n",
      "Training Accuracy per 5000 steps: 44.19642857142857\n",
      "Training Loss per 5000 steps: 2.0978262413107407\n",
      "Training Accuracy per 5000 steps: 44.354838709677416\n",
      "Training Loss per 5000 steps: 2.0870426416913896\n",
      "Training Accuracy per 5000 steps: 44.653179190751445\n",
      "Training Loss per 5000 steps: 2.0724468544507637\n",
      "Training Accuracy per 5000 steps: 45.085470085470085\n",
      "Training Loss per 5000 steps: 2.0611210881491724\n",
      "Training Accuracy per 5000 steps: 45.36516853932584\n",
      "Training Loss per 5000 steps: 2.0496682889573794\n",
      "Training Accuracy per 5000 steps: 45.70637119113574\n",
      "Training Loss per 5000 steps: 2.040153240734111\n",
      "Training Accuracy per 5000 steps: 46.03825136612022\n",
      "Training Loss per 5000 steps: 2.0401421799813964\n",
      "Training Accuracy per 5000 steps: 46.02425876010782\n",
      "Training Loss per 5000 steps: 2.0321438697741385\n",
      "Training Accuracy per 5000 steps: 46.1436170212766\n",
      "Training Loss per 5000 steps: 2.0271514547778553\n",
      "Training Accuracy per 5000 steps: 46.3254593175853\n",
      "Training Loss per 5000 steps: 2.027766100174405\n",
      "Training Accuracy per 5000 steps: 46.17875647668394\n",
      "Training Loss per 5000 steps: 2.0228394748609695\n",
      "Training Accuracy per 5000 steps: 46.35549872122762\n",
      "Training Loss per 5000 steps: 2.015355093762128\n",
      "Training Accuracy per 5000 steps: 46.6540404040404\n",
      "Training Loss per 5000 steps: 2.0100139648539765\n",
      "Training Accuracy per 5000 steps: 46.69576059850374\n",
      "Training Loss per 5000 steps: 2.000080410923277\n",
      "Training Accuracy per 5000 steps: 46.92118226600985\n",
      "Training Loss per 5000 steps: 1.9908352231022215\n",
      "Training Accuracy per 5000 steps: 47.141119221411195\n",
      "Training Loss per 5000 steps: 1.9841141416333044\n",
      "Training Accuracy per 5000 steps: 47.35576923076923\n",
      "Training Loss per 5000 steps: 1.9792422219721553\n",
      "Training Accuracy per 5000 steps: 47.50593824228029\n",
      "Training Loss per 5000 steps: 1.979669355380703\n",
      "Training Accuracy per 5000 steps: 47.47652582159625\n",
      "Training Loss per 5000 steps: 1.9780013891079586\n",
      "Training Accuracy per 5000 steps: 47.50580046403712\n",
      "Training Loss per 5000 steps: 1.975557921471399\n",
      "Training Accuracy per 5000 steps: 47.53440366972477\n",
      "Training Loss per 5000 steps: 1.9676290561958236\n",
      "Training Accuracy per 5000 steps: 47.90249433106576\n",
      "Training Loss per 5000 steps: 1.9591900886708846\n",
      "Training Accuracy per 5000 steps: 48.20627802690583\n",
      "Training Loss per 5000 steps: 1.958666754271133\n",
      "Training Accuracy per 5000 steps: 48.170731707317074\n",
      "Training Loss per 5000 steps: 1.958627565518806\n",
      "Training Accuracy per 5000 steps: 48.1359649122807\n",
      "Training Loss per 5000 steps: 1.9518925918674261\n",
      "Training Accuracy per 5000 steps: 48.31887201735358\n",
      "Training Loss per 5000 steps: 1.9475996670804823\n",
      "Training Accuracy per 5000 steps: 48.39055793991416\n",
      "Training Loss per 5000 steps: 1.9476545345504825\n",
      "Training Accuracy per 5000 steps: 48.40764331210191\n",
      "Training Loss per 5000 steps: 1.94015909005113\n",
      "Training Accuracy per 5000 steps: 48.686974789915965\n",
      "Training Loss per 5000 steps: 1.9378495871896804\n",
      "Training Accuracy per 5000 steps: 48.75259875259875\n",
      "Training Loss per 5000 steps: 1.9319408141045904\n",
      "Training Accuracy per 5000 steps: 48.91975308641975\n",
      "Training Loss per 5000 steps: 1.9201532019010144\n",
      "Training Accuracy per 5000 steps: 49.33808553971487\n",
      "Training Loss per 5000 steps: 1.9154239380431752\n",
      "Training Accuracy per 5000 steps: 49.546370967741936\n",
      "Training Loss per 5000 steps: 1.9076022043259082\n",
      "Training Accuracy per 5000 steps: 49.8502994011976\n",
      "Training Loss per 5000 steps: 1.8971372928190608\n",
      "Training Accuracy per 5000 steps: 50.148221343873516\n",
      "Training Loss per 5000 steps: 1.8869010592626732\n",
      "Training Accuracy per 5000 steps: 50.53816046966732\n",
      "Training Loss per 5000 steps: 1.886133227468461\n",
      "Training Accuracy per 5000 steps: 50.48449612403101\n",
      "Training Loss per 5000 steps: 1.8795864054657905\n",
      "Training Accuracy per 5000 steps: 50.67178502879079\n",
      "Training Loss per 5000 steps: 1.8769887485204995\n",
      "Training Accuracy per 5000 steps: 50.760456273764255\n",
      "Training Loss per 5000 steps: 1.8705996693639881\n",
      "Training Accuracy per 5000 steps: 50.941619585687384\n",
      "Training Loss per 5000 steps: 1.8614420427092866\n",
      "Training Accuracy per 5000 steps: 51.30597014925373\n",
      "Training Loss per 5000 steps: 1.8546691490629905\n",
      "Training Accuracy per 5000 steps: 51.43253234750462\n",
      "Training Loss per 5000 steps: 1.852358299624789\n",
      "Training Accuracy per 5000 steps: 51.46520146520147\n",
      "Training Loss per 5000 steps: 1.8483269582426483\n",
      "Training Accuracy per 5000 steps: 51.542649727767696\n",
      "Training Loss per 5000 steps: 1.8464305069056346\n",
      "Training Accuracy per 5000 steps: 51.57374100719424\n",
      "Training Loss per 5000 steps: 1.83991390004515\n",
      "Training Accuracy per 5000 steps: 51.7379679144385\n",
      "Training Loss per 5000 steps: 1.834153817183348\n",
      "Training Accuracy per 5000 steps: 51.81095406360424\n",
      "Training Loss per 5000 steps: 1.8294535423066278\n",
      "Training Accuracy per 5000 steps: 51.92644483362522\n",
      "Training Loss per 5000 steps: 1.8241554834724714\n",
      "Training Accuracy per 5000 steps: 52.03993055555556\n",
      "Training Loss per 5000 steps: 1.8154405507029847\n",
      "Training Accuracy per 5000 steps: 52.28055077452668\n",
      "Training Loss per 5000 steps: 1.81039758588572\n",
      "Training Accuracy per 5000 steps: 52.474402730375424\n",
      "Training Loss per 5000 steps: 1.808159897649147\n",
      "Training Accuracy per 5000 steps: 52.66497461928934\n",
      "Training Loss per 5000 steps: 1.8032267119410454\n",
      "Training Accuracy per 5000 steps: 52.76845637583892\n",
      "Training Loss per 5000 steps: 1.7946889447838614\n",
      "Training Accuracy per 5000 steps: 52.95341098169717\n",
      "Training Loss per 5000 steps: 1.7896650867542812\n",
      "Training Accuracy per 5000 steps: 53.09405940594059\n",
      "Training Loss per 5000 steps: 1.7857208152693345\n",
      "Training Accuracy per 5000 steps: 53.191489361702125\n",
      "Training Loss per 5000 steps: 1.7744926246874906\n",
      "Training Accuracy per 5000 steps: 53.57142857142857\n",
      "Training Loss per 5000 steps: 1.7752527045310984\n",
      "Training Accuracy per 5000 steps: 53.5829307568438\n",
      "Training Loss per 5000 steps: 1.7712278142095375\n",
      "Training Accuracy per 5000 steps: 53.79392971246006\n",
      "Training Loss per 5000 steps: 1.7672861698548503\n",
      "Training Accuracy per 5000 steps: 53.961965134706816\n",
      "Training Loss per 5000 steps: 1.7649250690057967\n",
      "Training Accuracy per 5000 steps: 54.009433962264154\n",
      "Training Loss per 5000 steps: 1.7614833811841406\n",
      "Training Accuracy per 5000 steps: 54.01716068642746\n",
      "Training Loss per 5000 steps: 1.7571426447100316\n",
      "Training Accuracy per 5000 steps: 54.140866873065015\n",
      "Training Loss per 5000 steps: 1.75182545191956\n",
      "Training Accuracy per 5000 steps: 54.3394777265745\n",
      "Training Loss per 5000 steps: 1.746627095518861\n",
      "Training Accuracy per 5000 steps: 54.573170731707314\n",
      "Training Loss per 5000 steps: 1.7468079994693286\n",
      "Training Accuracy per 5000 steps: 54.50075642965204\n",
      "Training Loss per 5000 steps: 1.7458870207090993\n",
      "Training Accuracy per 5000 steps: 54.57957957957958\n",
      "Training Loss per 5000 steps: 1.7381719458964826\n",
      "Training Accuracy per 5000 steps: 54.7317436661699\n",
      "Training Loss per 5000 steps: 1.7336406452724566\n",
      "Training Accuracy per 5000 steps: 54.844674556213015\n",
      "Training Loss per 5000 steps: 1.7314562427533706\n",
      "Training Accuracy per 5000 steps: 54.95594713656388\n",
      "Training Loss per 5000 steps: 1.7271720748123205\n",
      "Training Accuracy per 5000 steps: 55.065597667638485\n",
      "Training Loss per 5000 steps: 1.7249978206480292\n",
      "Training Accuracy per 5000 steps: 55.101302460202604\n",
      "Training Loss per 5000 steps: 1.7216777450608454\n",
      "Training Accuracy per 5000 steps: 55.208333333333336\n",
      "Training Loss per 5000 steps: 1.717275357174125\n",
      "Training Accuracy per 5000 steps: 55.313837375178316\n",
      "Training Loss per 5000 steps: 1.7135235457219415\n",
      "Training Accuracy per 5000 steps: 55.38243626062323\n",
      "Training Loss per 5000 steps: 1.7098892612170569\n",
      "Training Accuracy per 5000 steps: 55.48523206751055\n",
      "Training Loss per 5000 steps: 1.7071300627008164\n",
      "Training Accuracy per 5000 steps: 55.55167597765363\n",
      "Training Loss per 5000 steps: 1.7019520049989636\n",
      "Training Accuracy per 5000 steps: 55.68654646324549\n",
      "Training Loss per 5000 steps: 1.7003236770342534\n",
      "Training Accuracy per 5000 steps: 55.71625344352617\n",
      "Training Loss per 5000 steps: 1.697367596499897\n",
      "Training Accuracy per 5000 steps: 55.71135430916553\n",
      "Training Loss per 5000 steps: 1.6961865323271765\n",
      "Training Accuracy per 5000 steps: 55.70652173913044\n",
      "Training Loss per 5000 steps: 1.6945537600399834\n",
      "Training Accuracy per 5000 steps: 55.76923076923077\n",
      "Training Loss per 5000 steps: 1.6936486902529369\n",
      "Training Accuracy per 5000 steps: 55.86461126005362\n",
      "Training Loss per 5000 steps: 1.6906086180046618\n",
      "Training Accuracy per 5000 steps: 55.85885486018642\n",
      "Training Loss per 5000 steps: 1.6855073657340158\n",
      "Training Accuracy per 5000 steps: 55.95238095238095\n",
      "Training Loss per 5000 steps: 1.683182016250025\n",
      "Training Accuracy per 5000 steps: 55.97897503285151\n",
      "Training Loss per 5000 steps: 1.6769114207700089\n",
      "Training Accuracy per 5000 steps: 56.13577023498694\n",
      "Training Loss per 5000 steps: 1.6726096102390462\n",
      "Training Accuracy per 5000 steps: 56.25810635538262\n",
      "Training Loss per 5000 steps: 1.6687219816538477\n",
      "Training Accuracy per 5000 steps: 56.378865979381445\n",
      "Training Loss per 5000 steps: 1.6651344276039304\n",
      "Training Accuracy per 5000 steps: 56.49807938540333\n",
      "Training Loss per 5000 steps: 1.6620074799876783\n",
      "Training Accuracy per 5000 steps: 56.583969465648856\n",
      "Training Loss per 5000 steps: 1.6595373035004708\n",
      "Training Accuracy per 5000 steps: 56.605562579013906\n",
      "Training Loss per 5000 steps: 1.6547668253132446\n",
      "Training Accuracy per 5000 steps: 56.75251256281407\n",
      "Training Loss per 5000 steps: 1.6506628429472967\n",
      "Training Accuracy per 5000 steps: 56.86641697877653\n",
      "Training Loss per 5000 steps: 1.6481672888136383\n",
      "Training Accuracy per 5000 steps: 56.88585607940447\n",
      "Training Loss per 5000 steps: 1.6471096745769134\n",
      "Training Accuracy per 5000 steps: 56.90505548705302\n",
      "Training Loss per 5000 steps: 1.6439287294724994\n",
      "Training Accuracy per 5000 steps: 57.04656862745098\n",
      "Training Loss per 5000 steps: 1.6386823950020026\n",
      "Training Accuracy per 5000 steps: 57.1863580998782\n",
      "Training Loss per 5000 steps: 1.6365502599004393\n",
      "Training Accuracy per 5000 steps: 57.26392251815981\n",
      "Training Loss per 5000 steps: 1.6363655826173247\n",
      "Training Accuracy per 5000 steps: 57.250300842358605\n",
      "Training Loss per 5000 steps: 1.6346844646847989\n",
      "Training Accuracy per 5000 steps: 57.266746411483254\n",
      "Training Loss per 5000 steps: 1.6342278314614835\n",
      "Training Accuracy per 5000 steps: 57.25326991676575\n",
      "Training Loss per 5000 steps: 1.6319212417160083\n",
      "Training Accuracy per 5000 steps: 57.328605200945624\n",
      "Training Loss per 5000 steps: 1.6289781990048469\n",
      "Training Accuracy per 5000 steps: 57.40305522914218\n",
      "Training Loss per 5000 steps: 1.6257161730987446\n",
      "Training Accuracy per 5000 steps: 57.50584112149533\n",
      "Training Loss per 5000 steps: 1.622427278037132\n",
      "Training Accuracy per 5000 steps: 57.549361207897796\n",
      "Training Loss per 5000 steps: 1.6175391913905706\n",
      "Training Accuracy per 5000 steps: 57.70785219399538\n",
      "Training Loss per 5000 steps: 1.6124453422635896\n",
      "Training Accuracy per 5000 steps: 57.86452353616533\n",
      "Training Loss per 5000 steps: 1.6092038692642836\n",
      "Training Accuracy per 5000 steps: 57.87671232876713\n",
      "Training Loss per 5000 steps: 1.6072104452484446\n",
      "Training Accuracy per 5000 steps: 57.945516458569806\n",
      "Training Loss per 5000 steps: 1.6050799890309908\n",
      "Training Accuracy per 5000 steps: 57.92889390519187\n",
      "Training Loss per 5000 steps: 1.6023863451314964\n",
      "Training Accuracy per 5000 steps: 57.996632996633\n",
      "Training Loss per 5000 steps: 1.600213687334742\n",
      "Training Accuracy per 5000 steps: 58.035714285714285\n",
      "Training Loss per 5000 steps: 1.5979703918810557\n",
      "Training Accuracy per 5000 steps: 58.102108768035514\n",
      "Training Loss per 5000 steps: 1.5959906672799824\n",
      "Training Accuracy per 5000 steps: 58.1401766004415\n",
      "Training Loss per 5000 steps: 1.5908408939969265\n",
      "Training Accuracy per 5000 steps: 58.287596048298575\n",
      "Training Loss per 5000 steps: 1.5876449577868246\n",
      "Training Accuracy per 5000 steps: 58.37882096069869\n",
      "Training Loss per 5000 steps: 1.582465454418289\n",
      "Training Accuracy per 5000 steps: 58.469055374592834\n",
      "Training Loss per 5000 steps: 1.5773785891047565\n",
      "Training Accuracy per 5000 steps: 58.61231101511879\n",
      "Training Loss per 5000 steps: 1.5767929565989753\n",
      "Training Accuracy per 5000 steps: 58.673469387755105\n",
      "Training Loss per 5000 steps: 1.5762146735388753\n",
      "Training Accuracy per 5000 steps: 58.65384615384615\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[65], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(EPOCHS):\n\u001b[0;32m----> 2\u001b[0m     \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[64], line 28\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(epoch)\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining Accuracy per 5000 steps: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maccu_step\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     27\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 28\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# # When using GPU\u001b[39;00m\n\u001b[1;32m     30\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/zrive-ds-TXjvcAVs-py3.11/lib/python3.11/site-packages/torch/_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    521\u001b[0m     )\n\u001b[0;32m--> 522\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    524\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/zrive-ds-TXjvcAVs-py3.11/lib/python3.11/site-packages/torch/autograd/__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 266\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(EPOCHS):\n",
    "    train(epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Validation of the model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def valid(model, testing_loader):\n",
    "    model.eval()\n",
    "    n_correct = 0; n_wrong = 0; total = 0\n",
    "    with torch.no_grad():\n",
    "        for _, data in enumerate(testing_loader, 0):\n",
    "            ids = data['input_ids'].to(device, dtype = torch.long)\n",
    "            mask = data['attention_mask'].to(device, dtype = torch.long)\n",
    "            targets = data['labels'].to(device, dtype = torch.long).squeeze(1)\n",
    "            outputs = model(ids, mask).squeeze()\n",
    "            loss = loss_fn(outputs, targets)\n",
    "            tr_loss += loss.item()\n",
    "            big_val, big_idx = torch.max(outputs.data, dim=1)\n",
    "            n_correct += calcuate_accu(big_idx, targets)\n",
    "\n",
    "            nb_tr_steps += 1\n",
    "            nb_tr_examples+=targets.size(0)\n",
    "            \n",
    "            if _%5000==0:\n",
    "                loss_step = tr_loss/nb_tr_steps\n",
    "                accu_step = (n_correct*100)/nb_tr_examples\n",
    "                print(f\"Validation Loss per 100 steps: {loss_step}\")\n",
    "                print(f\"Validation Accuracy per 100 steps: {accu_step}\")\n",
    "    epoch_loss = tr_loss/nb_tr_steps\n",
    "    epoch_accu = (n_correct*100)/nb_tr_examples\n",
    "    print(f\"Validation Loss Epoch: {epoch_loss}\")\n",
    "    print(f\"Validation Accuracy Epoch: {epoch_accu}\")\n",
    "    \n",
    "    return epoch_accu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('This is the validation section to print the accuracy and see how it performs')\n",
    "print('Here we are leveraging on the dataloader crearted for the validation dataset, the approcah is using more of pytorch')\n",
    "\n",
    "acc = valid(model, val_loader)\n",
    "print(\"Accuracy on val data = %0.2f%%\" % acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SEPARATE GET EMBEDDINGS AND MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import DistilBertTokenizer, DistilBertModel\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, dataframe, max_len):\n",
    "        self.tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "        self.texts = np.array(dataframe['BUSINESS_DESCRIPTION'].astype(str))\n",
    "        self.targets = np.array(dataframe['label'])\n",
    "        self.max_len = max_len\n",
    "        self.model = DistilBertModel.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "        for param in self.model.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        target = self.targets[idx]\n",
    "\n",
    "        # Tokenizar el texto y obtener los embeddings con el modelo DistilBERT\n",
    "        inputs = self.tokenizer(text, return_tensors='pt', padding=True, truncation=True, max_length=self.max_len)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(**inputs)\n",
    "\n",
    "        # Obtener los embeddings del token [CLS] (última capa oculta)\n",
    "        embeddings = outputs.last_hidden_state[:, 0, :]  # Embeddings del token [CLS]\n",
    "\n",
    "        label = torch.tensor(target, dtype=torch.long)\n",
    "\n",
    "        return {\n",
    "            'embeddings': embeddings,\n",
    "            'label': label\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DistilBertModel.from_pretrained(\"distilbert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Module.parameters of DistilBertModel(\n",
       "  (embeddings): Embeddings(\n",
       "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (transformer): Transformer(\n",
       "    (layer): ModuleList(\n",
       "      (0-5): 6 x TransformerBlock(\n",
       "        (attention): MultiHeadSelfAttention(\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (ffn): FFN(\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (activation): GELUActivation()\n",
       "        )\n",
       "        (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")>"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = CustomDataset(train_data,  MAX_LEN)\n",
    "val_set = CustomDataset(val_data, MAX_LEN)\n",
    "test_set = CustomDataset(test_data, MAX_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set[0]['embeddings'].numpy().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    \n",
    "    embeddings = torch.stack([item['embeddings'].squeeze(0) for item in batch])\n",
    "    labels = torch.tensor([item['label'] for item in batch])\n",
    "\n",
    "    return {'embeddings': embeddings, 'label': labels}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_loader = DataLoader(dataset=training_set, batch_size=TRAIN_BATCH_SIZE, shuffle=True, num_workers=0, collate_fn=collate_fn)\n",
    "val_loader = DataLoader(dataset=val_set, batch_size=VALID_BATCH_SIZE, shuffle=True, num_workers=0, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.utils.data.dataloader.DataLoader'>\n",
      "['_DataLoader__initialized', '_DataLoader__multiprocessing_context', '_IterableDataset_len_called', '__annotations__', '__class__', '__class_getitem__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__iter__', '__le__', '__len__', '__lt__', '__module__', '__ne__', '__new__', '__orig_bases__', '__parameters__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__slots__', '__str__', '__subclasshook__', '__weakref__', '_auto_collation', '_dataset_kind', '_get_iterator', '_index_sampler', '_is_protocol', '_iterator', 'batch_sampler', 'batch_size', 'check_worker_number_rationality', 'collate_fn', 'dataset', 'drop_last', 'generator', 'multiprocessing_context', 'num_workers', 'persistent_workers', 'pin_memory', 'pin_memory_device', 'prefetch_factor', 'sampler', 'timeout', 'worker_init_fn']\n",
      "Cantidad de lotes en training_loader: 319\n",
      "embeddings: torch.Size([4, 768])\n",
      "labels: tensor([13, 10, 16, 12])\n"
     ]
    }
   ],
   "source": [
    "print(type(val_loader))\n",
    "print(dir(val_loader))\n",
    "print(\"Cantidad de lotes en training_loader:\", len(val_loader))\n",
    "\n",
    "for i, batch in enumerate(training_loader):\n",
    "    print(f\"embeddings: {batch['embeddings'].shape}\")\n",
    "    print(f\"labels: {batch['label']}\")\n",
    "\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "import torch.nn as nn\n",
    "\n",
    "class MLPClassifier(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, dropout_prob=0.1):\n",
    "        super(MLPClassifier, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, input_size)\n",
    "        self.dropout = nn.Dropout(dropout_prob)  # Capa de Dropout\n",
    "        self.fc2 = nn.Linear(input_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.dropout(x)  # Aplicar Dropout después de la función de activación\n",
    "        x = self.fc2(x)\n",
    "        return x  # La salida será de la forma [batch_size, output_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_model = MLPClassifier(input_size=768, hidden_size=256, output_size=24)\n",
    "mlp_model.to(device)\n",
    "\n",
    "def loss_fn(outputs, targets):\n",
    "    return torch.nn.CrossEntropyLoss()(outputs, targets)\n",
    "\n",
    "optimizer = torch.optim.Adam(params =  mlp_model.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, optimizer, loss_fn, device, epochs):\n",
    "    \n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        #model.train()\n",
    "        tr_loss = 0.0\n",
    "        correct_preds = 0\n",
    "        total_examples = 0\n",
    "\n",
    "        for batch_idx, data in enumerate(train_loader, 1):\n",
    "            inputs = data['embeddings'].to(device)  # Input data tensor\n",
    "            targets = data['label'].to(device)  # Target label tensor\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            # Compute loss\n",
    "            loss = loss_fn(outputs, targets)\n",
    "            tr_loss += loss.item()\n",
    "\n",
    "            # Backward pass\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Compute accuracy\n",
    "            correct_preds += torch.sum(torch.argmax(outputs, dim=1) == targets).item()\n",
    "            total_examples += targets.size(0)\n",
    "\n",
    "            if batch_idx % 5 == 0:\n",
    "                avg_loss = tr_loss / batch_idx\n",
    "                accuracy = correct_preds / total_examples * 100\n",
    "                print(f\"Epoch [{epoch}/{epochs}], Step [{batch_idx}/{len(train_loader)}], \"\n",
    "                      f\"Average Loss: {avg_loss:.4f}, Accuracy: {accuracy:.2f}%\")\n",
    "\n",
    "        # Calculate epoch-level metrics\n",
    "        epoch_loss = tr_loss / len(train_loader)\n",
    "        epoch_accuracy = correct_preds / total_examples * 100\n",
    "\n",
    "        print(f\"Epoch [{epoch}/{epochs}], Train Loss: {epoch_loss:.4f}, Train Accuracy: {epoch_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(mlp_model, training_loader, optimizer, loss_fn, device='cuda' if torch.cuda.is_available() else 'cpu', epochs=EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_params_distilbert = sum(p.numel() for p in mlp_model.parameters() if p.requires_grad)\n",
    "print(f\"Total trainable parameters in DistilBERTClass: {total_params_distilbert}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next DistilBERT aprox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Module.parameters of DistilBertModel(\n",
       "  (embeddings): Embeddings(\n",
       "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (transformer): Transformer(\n",
       "    (layer): ModuleList(\n",
       "      (0-5): 6 x TransformerBlock(\n",
       "        (attention): MultiHeadSelfAttention(\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (ffn): FFN(\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (activation): GELUActivation()\n",
       "        )\n",
       "        (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")>"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = DistilBertModel.from_pretrained(\"distilbert-base-uncased\")\n",
    "model.parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_hidden_states(batch):\n",
    "    inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import DistilBertTokenizer, DistilBertModel\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, dataframe, max_len):\n",
    "        self.tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "        self.texts = np.array(dataframe['BUSINESS_DESCRIPTION'].astype(str))\n",
    "        self.targets = np.array(dataframe['label'])\n",
    "        self.max_len = max_len\n",
    "        self.model = DistilBertModel.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "        for param in self.model.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        target = self.targets[idx]\n",
    "\n",
    "        # Tokenizar el texto y obtener los embeddings con el modelo DistilBERT\n",
    "        inputs = self.tokenizer(text, return_tensors='pt', padding=True, truncation=True, max_length=self.max_len)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(**inputs)\n",
    "\n",
    "        # Obtener los embeddings del token [CLS] (última capa oculta)\n",
    "        embeddings = outputs.last_hidden_state[:, 0, :]  # Embeddings del token [CLS]\n",
    "\n",
    "        label = torch.tensor(target, dtype=torch.long)\n",
    "\n",
    "        return {\n",
    "            'embeddings': embeddings,\n",
    "            'label': label\n",
    "        }\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "zrive-ds-TXjvcAVs-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
