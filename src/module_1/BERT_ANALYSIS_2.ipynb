{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/edu/.cache/pypoetry/virtualenvs/zrive-ds-J_lydEot-py3.11/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import logging\n",
    "import torch\n",
    "from datasets import Dataset, DatasetDict\n",
    "from transformers import AutoTokenizer, DistilBertTokenizer\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import Dataset, DatasetDict\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(levelname)s: %(message)s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '../data/coverwallet.xlsx'\n",
    "df = pd.read_excel(data_path)\n",
    "df = df.dropna()\n",
    "def truncate_naics_and_prepare_data(df, column_name, num_digits):\n",
    "    \"\"\"\n",
    "    Truncates the NAICS codes in the specified column to the desired number of digits.\n",
    "\n",
    "    :param df: pandas DataFrame containing the NAICS codes.\n",
    "    :param column_name: the name of the column with the NAICS codes.\n",
    "    :param num_digits: the number of digits to truncate to.\n",
    "    :return: A copy of the DataFrame with the NAICS codes truncated.\n",
    "    \"\"\"\n",
    "    # Validate the number of digits\n",
    "    if not isinstance(num_digits, int) or num_digits <= 0:\n",
    "        logging.error(\"Number of digits must be a positive integer\")\n",
    "        raise ValueError(\"Number of digits must be a positive integer\")\n",
    "    \n",
    "    # Make a copy of the DataFrame to avoid modifying the original\n",
    "    df_copy = df.copy()\n",
    "    def truncate_code(code):\n",
    "        \"\"\"\n",
    "        Truncates or pads the NAICS code to the specified number of digits.\n",
    "        :param code: the NAICS code to be truncated.\n",
    "        :return: The truncated or original NAICS code as a string.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Ensure the code is a string\n",
    "            code_str = str(code)\n",
    "            # Truncate the code if it's longer than num_digits\n",
    "            return code_str[:num_digits].ljust(num_digits, '0')\n",
    "        except Exception as e:\n",
    "            logging.exception(\"Error truncating code: {}\".format(code))\n",
    "            return code\n",
    "        \n",
    "    # Apply the truncation function to the specified column\n",
    "    df_copy[column_name] = df_copy[column_name].apply(truncate_code)\n",
    "    # Try to convert the truncated column to integers\n",
    "    try:\n",
    "        df_copy[column_name] = df_copy[column_name].astype(int)\n",
    "    except ValueError as e:\n",
    "        logging.warning(\"Could not convert truncated codes to integers: {}\".format(e))\n",
    "        # Keep the column as strings if conversion fails\n",
    "        pass\n",
    "    \n",
    "    labels = df_copy['NAICS'].unique().tolist()\n",
    "    id2label = {idx: label for idx, label in enumerate(labels)}\n",
    "    label2id = {label: idx for idx, label in enumerate(labels)}\n",
    "    df_copy['label'] = df_copy['NAICS'].map(label2id)\n",
    "    logging.info(\"NAICS codes processed successfully. Here's the head of the processed DataFrame:\")\n",
    "    logging.info(\"\\n%s\", df_copy.head())\n",
    "    df_copy_train, df_copy_final_val = train_test_split(df_copy, test_size=0.15, shuffle=True, random_state=42)\n",
    "    \n",
    "    dataset_train = Dataset.from_pandas(df_copy_train)\n",
    "    dataset_final_val = Dataset.from_pandas(df_copy_final_val)\n",
    "\n",
    "# Configuration k-fold\n",
    "    num_folds = 3\n",
    "    kf = KFold(n_splits=num_folds, shuffle=True, random_state=42)\n",
    "    kfold_datasets = []\n",
    "\n",
    "    for fold, (train_indices, val_indices) in enumerate(kf.split(dataset_train)):\n",
    "        train_dataset = dataset_train.select(train_indices)\n",
    "        val_dataset = dataset_train.select(val_indices)\n",
    "        \n",
    "        dataset_dict = {\n",
    "            'train': train_dataset,\n",
    "            'validation': val_dataset\n",
    "        }\n",
    "\n",
    "        features_dict = {\n",
    "            \"NAICS\": dataset_train[\"NAICS\"],\n",
    "            \"BUSINESS_DESCRIPTION\": dataset_train[\"BUSINESS_DESCRIPTION\"],\n",
    "        }\n",
    "    \n",
    "        kfold_datasets.append(dataset_dict)\n",
    "        logging.info(f\"Processed fold {fold + 1}\")\n",
    "\n",
    "    for i, dataset_dict in enumerate(kfold_datasets):\n",
    "        for split in dataset_dict.keys():\n",
    "            dataset_dict[split] = dataset_dict[split].map(lambda example: {key: example[key] for key in features_dict.keys()})\n",
    "\n",
    "        logging.info(f\"DatasetDict for Fold {i + 1}:\")\n",
    "        for split, dataset in dataset_dict.items():\n",
    "            logging.info(f\"  {split} split: {dataset}\")\n",
    "            \n",
    "    logging.info(\"NAICS codes truncated successfully. Here's the head of the truncated DataFrame:\")\n",
    "    logging.info(\"\\n%s\", df_copy.head())\n",
    "    logging.info(\"Number of unique NAICS labels: %d\", len(labels))\n",
    "\n",
    "    return df_copy, kfold_datasets, dataset_train, dataset_final_val\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: NAICS codes processed successfully. Here's the head of the processed DataFrame:\n",
      "INFO: \n",
      "   NAICS                               BUSINESS_DESCRIPTION  label\n",
      "0     72  Zenyai Viet Cajun & Pho Restaurant is dedicate...      0\n",
      "1     54  Kilduff Underground Engineering, Inc. (KUE) is...      1\n",
      "2     45  024™ is a premium home fragrance brand that de...      2\n",
      "3     56  Our Services include Office Cleaning Carpet cl...      3\n",
      "4     62                    NYS Licensed Home Health Agency      4\n",
      "INFO: Processed fold 1\n",
      "INFO: Processed fold 2\n",
      "INFO: Processed fold 3\n",
      "Map: 100%|██████████| 8032/8032 [00:00<00:00, 8226.70 examples/s]\n",
      "Map: 100%|██████████| 4016/4016 [00:00<00:00, 6286.37 examples/s]\n",
      "INFO: DatasetDict for Fold 1:\n",
      "INFO:   train split: Dataset({\n",
      "    features: ['NAICS', 'BUSINESS_DESCRIPTION', 'label', '__index_level_0__'],\n",
      "    num_rows: 8032\n",
      "})\n",
      "INFO:   validation split: Dataset({\n",
      "    features: ['NAICS', 'BUSINESS_DESCRIPTION', 'label', '__index_level_0__'],\n",
      "    num_rows: 4016\n",
      "})\n",
      "Map: 100%|██████████| 8032/8032 [00:01<00:00, 7962.12 examples/s]\n",
      "Map: 100%|██████████| 4016/4016 [00:00<00:00, 7943.50 examples/s]\n",
      "INFO: DatasetDict for Fold 2:\n",
      "INFO:   train split: Dataset({\n",
      "    features: ['NAICS', 'BUSINESS_DESCRIPTION', 'label', '__index_level_0__'],\n",
      "    num_rows: 8032\n",
      "})\n",
      "INFO:   validation split: Dataset({\n",
      "    features: ['NAICS', 'BUSINESS_DESCRIPTION', 'label', '__index_level_0__'],\n",
      "    num_rows: 4016\n",
      "})\n",
      "Map: 100%|██████████| 8032/8032 [00:01<00:00, 7315.04 examples/s]\n",
      "Map: 100%|██████████| 4016/4016 [00:00<00:00, 6946.63 examples/s]\n",
      "INFO: DatasetDict for Fold 3:\n",
      "INFO:   train split: Dataset({\n",
      "    features: ['NAICS', 'BUSINESS_DESCRIPTION', 'label', '__index_level_0__'],\n",
      "    num_rows: 8032\n",
      "})\n",
      "INFO:   validation split: Dataset({\n",
      "    features: ['NAICS', 'BUSINESS_DESCRIPTION', 'label', '__index_level_0__'],\n",
      "    num_rows: 4016\n",
      "})\n",
      "INFO: NAICS codes truncated successfully. Here's the head of the truncated DataFrame:\n",
      "INFO: \n",
      "   NAICS                               BUSINESS_DESCRIPTION  label\n",
      "0     72  Zenyai Viet Cajun & Pho Restaurant is dedicate...      0\n",
      "1     54  Kilduff Underground Engineering, Inc. (KUE) is...      1\n",
      "2     45  024™ is a premium home fragrance brand that de...      2\n",
      "3     56  Our Services include Office Cleaning Carpet cl...      3\n",
      "4     62                    NYS Licensed Home Health Agency      4\n",
      "INFO: Number of unique NAICS labels: 24\n"
     ]
    }
   ],
   "source": [
    "df_2_digits, kfold_2_digits, dataset_train_2_digits, dataset_final_val_2_digits = truncate_naics_and_prepare_data(df, 'NAICS', 2)\n",
    "# Defining some key variables that will be used later on in the training\n",
    "MAX_LEN = 128 \n",
    "TRAIN_BATCH_SIZE = 4\n",
    "VALID_BATCH_SIZE = 4\n",
    "EPOCHS = 2\n",
    "LEARNING_RATE = 1e-05\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased', truncation=True, do_lower_case=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CREACION CLASE DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "\n",
    "class MultiLabelDataset(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer, max_len):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.text = dataframe['BUSINESS_DESCRIPTION']\n",
    "        self.targets = dataframe['label']\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.text)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.text[idx]\n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            text, None,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "        )\n",
    "        return {\n",
    "            'input_ids': torch.tensor(inputs['input_ids'], dtype=torch.long),\n",
    "            'attention_mask': torch.tensor(inputs['attention_mask'], dtype=torch.long),\n",
    "            'labels': torch.tensor(self.targets[idx], dtype=torch.float)\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DATALOADERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def create_dataloaders(kfold_datasets, tokenizer, max_len, batch_size=4):\n",
    "    dataloaders = []\n",
    "    for fold in kfold_datasets:\n",
    "        train_dataset = MultiLabelDataset(fold['train'], tokenizer, max_len)\n",
    "        valid_dataset = MultiLabelDataset(fold['validation'], tokenizer, max_len)\n",
    "        \n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "        valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False)\n",
    "        \n",
    "        dataloaders.append((train_loader, valid_loader))\n",
    "    \n",
    "    return dataloaders\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MODEL DEFINITION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DistilBertModel\n",
    "import torch.nn as nn\n",
    "\n",
    "class DistilBERTClass(nn.Module):\n",
    "    def __init__(self, num_labels):\n",
    "        super(DistilBERTClass, self).__init__()\n",
    "        self.distilbert = DistilBertModel.from_pretrained('distilbert-base-uncased')\n",
    "        self.pre_classifier = nn.Linear(768, 768)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.classifier = nn.Linear(768, num_labels)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        distilbert_output = self.distilbert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        hidden_state = distilbert_output[0]\n",
    "        pooled_output = hidden_state[:, 0]\n",
    "        pooled_output = self.pre_classifier(pooled_output)\n",
    "        pooled_output = nn.ReLU()(pooled_output)\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        output = self.classifier(pooled_output)\n",
    "        return output\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import Adam\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Definición de la función de pérdida\n",
    "loss_fn = nn.BCEWithLogitsLoss()\n",
    "def train_model(model, data_loader, loss_fn, optimizer, device, epochs):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for batch in tqdm(data_loader, desc=f\"Epoch {epoch + 1}/{epochs}\", leave=False):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            loss = loss_fn(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        avg_loss = total_loss / len(data_loader)\n",
    "        print(f\"Epoch {epoch + 1} finished. Average loss: {avg_loss:.4f}\")\n",
    "\n",
    "def validate_model(model, data_loader, loss_fn, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(data_loader, desc=\"Validating\", leave=False):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            loss = loss_fn(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "    \n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    print(f\"Validation finished. Average loss: {avg_loss:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/edu/.cache/pypoetry/virtualenvs/zrive-ds-J_lydEot-py3.11/lib/python3.11/site-packages/transformers/optimization.py:457: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "string indices must be integers, not 'str'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 14\u001b[0m\n\u001b[1;32m     11\u001b[0m loss_fn \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mBCEWithLogitsLoss()\n\u001b[1;32m     12\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m AdamW(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5e-5\u001b[39m)\n\u001b[0;32m---> 14\u001b[0m train_dataloader, valid_dataloader \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_dataloaders\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkfold_2_digits\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_len\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m128\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m16\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Asume que kfold_datasets[0] contiene tu primer fold.\u001b[39;00m\n\u001b[1;32m     16\u001b[0m train_model(model, train_dataloader, loss_fn, optimizer, device, EPOCHS)\n\u001b[1;32m     17\u001b[0m validate_model(model, valid_dataloader, loss_fn, device)\n",
      "Cell \u001b[0;32mIn[5], line 6\u001b[0m, in \u001b[0;36mcreate_dataloaders\u001b[0;34m(kfold_datasets, tokenizer, max_len, batch_size)\u001b[0m\n\u001b[1;32m      4\u001b[0m dataloaders \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m fold \u001b[38;5;129;01min\u001b[39;00m kfold_datasets:\n\u001b[0;32m----> 6\u001b[0m     train_dataset \u001b[38;5;241m=\u001b[39m MultiLabelDataset(\u001b[43mfold\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m, tokenizer, max_len)\n\u001b[1;32m      7\u001b[0m     valid_dataset \u001b[38;5;241m=\u001b[39m MultiLabelDataset(fold[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvalidation\u001b[39m\u001b[38;5;124m'\u001b[39m], tokenizer, max_len)\n\u001b[1;32m      9\u001b[0m     train_loader \u001b[38;5;241m=\u001b[39m DataLoader(train_dataset, batch_size\u001b[38;5;241m=\u001b[39mbatch_size, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mTypeError\u001b[0m: string indices must be integers, not 'str'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AdamW\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Asumiendo que model es tu instancia de DistilBERTClass\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = DistilBERTClass(num_labels=24)\n",
    "model.to(device)\n",
    "\n",
    "# Definición de la función de pérdida y el optimizador\n",
    "loss_fn = torch.nn.BCEWithLogitsLoss()\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "train_dataloader, valid_dataloader = create_dataloaders(kfold_2_digits[0], tokenizer, max_len=128, batch_size=16)  # Asume que kfold_datasets[0] contiene tu primer fold.\n",
    "\n",
    "train_model(model, train_dataloader, loss_fn, optimizer, device, EPOCHS)\n",
    "validate_model(model, valid_dataloader, loss_fn, device)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "zrive-ds-J_lydEot-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
