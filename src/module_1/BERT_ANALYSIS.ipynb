{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT ANALYSIS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LIBRARIES LOADING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/edu/.cache/pypoetry/virtualenvs/zrive-ds-J_lydEot-py3.11/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import logging\n",
    "import torch\n",
    "from datasets import Dataset, DatasetDict\n",
    "from transformers import AutoTokenizer, DistilBertTokenizer\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import Dataset, DatasetDict\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(levelname)s: %(message)s')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This approach will be very useful not only for going step by step with the classification but also for allowing to decide the trade-off of accuracy and digits returned."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DATA PREPROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '../data/coverwallet.xlsx'\n",
    "df = pd.read_excel(data_path)\n",
    "df = df.dropna()\n",
    "def truncate_naics_and_prepare_data(df, column_name, num_digits):\n",
    "    \"\"\"\n",
    "    Truncates the NAICS codes in the specified column to the desired number of digits.\n",
    "\n",
    "    :param df: pandas DataFrame containing the NAICS codes.\n",
    "    :param column_name: the name of the column with the NAICS codes.\n",
    "    :param num_digits: the number of digits to truncate to.\n",
    "    :return: A copy of the DataFrame with the NAICS codes truncated.\n",
    "    \"\"\"\n",
    "    # Validate the number of digits\n",
    "    if not isinstance(num_digits, int) or num_digits <= 0:\n",
    "        logging.error(\"Number of digits must be a positive integer\")\n",
    "        raise ValueError(\"Number of digits must be a positive integer\")\n",
    "    \n",
    "    # Make a copy of the DataFrame to avoid modifying the original\n",
    "    df_copy = df.copy()\n",
    "    def truncate_code(code):\n",
    "        \"\"\"\n",
    "        Truncates or pads the NAICS code to the specified number of digits.\n",
    "        :param code: the NAICS code to be truncated.\n",
    "        :return: The truncated or original NAICS code as a string.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Ensure the code is a string\n",
    "            code_str = str(code)\n",
    "            # Truncate the code if it's longer than num_digits\n",
    "            return code_str[:num_digits].ljust(num_digits, '0')\n",
    "        except Exception as e:\n",
    "            logging.exception(\"Error truncating code: {}\".format(code))\n",
    "            return code\n",
    "        \n",
    "    # Apply the truncation function to the specified column\n",
    "    df_copy[column_name] = df_copy[column_name].apply(truncate_code)\n",
    "    # Try to convert the truncated column to integers\n",
    "    try:\n",
    "        df_copy[column_name] = df_copy[column_name].astype(int)\n",
    "    except ValueError as e:\n",
    "        logging.warning(\"Could not convert truncated codes to integers: {}\".format(e))\n",
    "        # Keep the column as strings if conversion fails\n",
    "        pass\n",
    "    \n",
    "    labels = df_copy['NAICS'].unique().tolist()\n",
    "    id2label = {idx: label for idx, label in enumerate(labels)}\n",
    "    label2id = {label: idx for idx, label in enumerate(labels)}\n",
    "    df_copy['label'] = df_copy['NAICS'].map(label2id)\n",
    "    logging.info(\"NAICS codes processed successfully. Here's the head of the processed DataFrame:\")\n",
    "    logging.info(\"\\n%s\", df_copy.head())\n",
    "    df_copy_train, df_copy_final_val = train_test_split(df_copy, test_size=0.15, shuffle=True, random_state=42)\n",
    "    \n",
    "    dataset_train = Dataset.from_pandas(df_copy_train)\n",
    "    dataset_final_val = Dataset.from_pandas(df_copy_final_val)\n",
    "\n",
    "# Configuration k-fold\n",
    "    num_folds = 3\n",
    "    kf = KFold(n_splits=num_folds, shuffle=True, random_state=42)\n",
    "    kfold_datasets = []\n",
    "\n",
    "    for fold, (train_indices, val_indices) in enumerate(kf.split(dataset_train)):\n",
    "        train_dataset = dataset_train.select(train_indices)\n",
    "        val_dataset = dataset_train.select(val_indices)\n",
    "        \n",
    "        dataset_dict = {\n",
    "            'train': train_dataset,\n",
    "            'validation': val_dataset\n",
    "        }\n",
    "\n",
    "        features_dict = {\n",
    "            \"NAICS\": dataset_train[\"NAICS\"],\n",
    "            \"BUSINESS_DESCRIPTION\": dataset_train[\"BUSINESS_DESCRIPTION\"],\n",
    "        }\n",
    "    \n",
    "        kfold_datasets.append(dataset_dict)\n",
    "        logging.info(f\"Processed fold {fold + 1}\")\n",
    "\n",
    "    for i, dataset_dict in enumerate(kfold_datasets):\n",
    "        for split in dataset_dict.keys():\n",
    "            dataset_dict[split] = dataset_dict[split].map(lambda example: {key: example[key] for key in features_dict.keys()})\n",
    "\n",
    "        logging.info(f\"DatasetDict for Fold {i + 1}:\")\n",
    "        for split, dataset in dataset_dict.items():\n",
    "            logging.info(f\"  {split} split: {dataset}\")\n",
    "            \n",
    "    logging.info(\"NAICS codes truncated successfully. Here's the head of the truncated DataFrame:\")\n",
    "    logging.info(\"\\n%s\", df_copy.head())\n",
    "    logging.info(\"Number of unique NAICS labels: %d\", len(labels))\n",
    "\n",
    "    return df_copy, kfold_datasets, dataset_train, dataset_final_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: NAICS codes processed successfully. Here's the head of the processed DataFrame:\n",
      "INFO: \n",
      "   NAICS                               BUSINESS_DESCRIPTION  label\n",
      "0     72  Zenyai Viet Cajun & Pho Restaurant is dedicate...      0\n",
      "1     54  Kilduff Underground Engineering, Inc. (KUE) is...      1\n",
      "2     45  024™ is a premium home fragrance brand that de...      2\n",
      "3     56  Our Services include Office Cleaning Carpet cl...      3\n",
      "4     62                    NYS Licensed Home Health Agency      4\n",
      "INFO: Processed fold 1\n",
      "INFO: Processed fold 2\n",
      "INFO: Processed fold 3\n",
      "Map: 100%|██████████| 8032/8032 [00:02<00:00, 3054.00 examples/s]\n",
      "Map: 100%|██████████| 4016/4016 [00:01<00:00, 2656.07 examples/s]\n",
      "INFO: DatasetDict for Fold 1:\n",
      "INFO:   train split: Dataset({\n",
      "    features: ['NAICS', 'BUSINESS_DESCRIPTION', 'label', '__index_level_0__'],\n",
      "    num_rows: 8032\n",
      "})\n",
      "INFO:   validation split: Dataset({\n",
      "    features: ['NAICS', 'BUSINESS_DESCRIPTION', 'label', '__index_level_0__'],\n",
      "    num_rows: 4016\n",
      "})\n",
      "Map: 100%|██████████| 8032/8032 [00:02<00:00, 2792.98 examples/s]\n",
      "Map: 100%|██████████| 4016/4016 [00:01<00:00, 3289.70 examples/s]\n",
      "INFO: DatasetDict for Fold 2:\n",
      "INFO:   train split: Dataset({\n",
      "    features: ['NAICS', 'BUSINESS_DESCRIPTION', 'label', '__index_level_0__'],\n",
      "    num_rows: 8032\n",
      "})\n",
      "INFO:   validation split: Dataset({\n",
      "    features: ['NAICS', 'BUSINESS_DESCRIPTION', 'label', '__index_level_0__'],\n",
      "    num_rows: 4016\n",
      "})\n",
      "Map: 100%|██████████| 8032/8032 [00:02<00:00, 2997.06 examples/s]\n",
      "Map: 100%|██████████| 4016/4016 [00:01<00:00, 3176.64 examples/s]\n",
      "INFO: DatasetDict for Fold 3:\n",
      "INFO:   train split: Dataset({\n",
      "    features: ['NAICS', 'BUSINESS_DESCRIPTION', 'label', '__index_level_0__'],\n",
      "    num_rows: 8032\n",
      "})\n",
      "INFO:   validation split: Dataset({\n",
      "    features: ['NAICS', 'BUSINESS_DESCRIPTION', 'label', '__index_level_0__'],\n",
      "    num_rows: 4016\n",
      "})\n",
      "INFO: NAICS codes truncated successfully. Here's the head of the truncated DataFrame:\n",
      "INFO: \n",
      "   NAICS                               BUSINESS_DESCRIPTION  label\n",
      "0     72  Zenyai Viet Cajun & Pho Restaurant is dedicate...      0\n",
      "1     54  Kilduff Underground Engineering, Inc. (KUE) is...      1\n",
      "2     45  024™ is a premium home fragrance brand that de...      2\n",
      "3     56  Our Services include Office Cleaning Carpet cl...      3\n",
      "4     62                    NYS Licensed Home Health Agency      4\n",
      "INFO: Number of unique NAICS labels: 24\n"
     ]
    }
   ],
   "source": [
    "df_2_digits, kfold_2_digits, dataset_train_2_digits, dataset_final_val_2_digits = truncate_naics_and_prepare_data(df, 'NAICS', 2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining some key variables that will be used later on in the training\n",
    "MAX_LEN = 128 \n",
    "TRAIN_BATCH_SIZE = 4\n",
    "VALID_BATCH_SIZE = 4\n",
    "EPOCHS = 2\n",
    "LEARNING_RATE = 1e-05\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased', truncation=True, do_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NAICSDataset(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer, max_len):\n",
    "        \"\"\"\n",
    "        Inicializa el dataset.\n",
    "\n",
    "        :param dataframe: DataFrame de pandas que contiene las descripciones de negocio y sus etiquetas.\n",
    "        :param tokenizer: Tokenizador de transformers utilizado para procesar el texto.\n",
    "        :param max_len: Longitud máxima de la secuencia de tokens.\n",
    "        \"\"\"\n",
    "        self.tokenizer = tokenizer\n",
    "        self.data = dataframe\n",
    "        self.text = dataframe['BUSINESS_DESCRIPTION']  \n",
    "        self.targets = list(dataframe['labels'])  \n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Devuelve la cantidad de elementos en el dataset.\n",
    "        \"\"\"\n",
    "        return len(self.text)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        Obtiene el ítem en el índice especificado con tokenización y preparación para el modelo.\n",
    "        \"\"\"\n",
    "        # Extract text and basic cleaning\n",
    "        text = str(self.text[index])\n",
    "        text = \" \".join(text.split())\n",
    "\n",
    "        # Codificar el texto\n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            None,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_token_type_ids=False  # DistilBERT doesn not use token_type_ids\n",
    "        )\n",
    "\n",
    "        # Preparar la salida\n",
    "        ids = inputs['input_ids']\n",
    "        mask = inputs['attention_mask']\n",
    "\n",
    "        return {\n",
    "            'ids': torch.tensor(ids, dtype=torch.long),\n",
    "            'mask': torch.tensor(mask, dtype=torch.long),\n",
    "            'targets': torch.tensor(self.targets[index], dtype=torch.float)  # Asegúrate de que esto coincide con el formato esperado por tu modelo\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data_loaders_from_datasets(train_dataset, valid_dataset, tokenizer, max_len, train_batch_size, valid_batch_size):\n",
    "    \"\"\"\n",
    "    Crea DataLoader para los conjuntos de datos de entrenamiento y validación.\n",
    "\n",
    "    :param train_dataset: DataFrame de entrenamiento o NAICSDataset ya inicializado.\n",
    "    :param valid_dataset: DataFrame de validación o NAICSDataset ya inicializado.\n",
    "    :param tokenizer: Tokenizador de transformers utilizado para procesar el texto.\n",
    "    :param max_len: Longitud máxima de la secuencia de tokens.\n",
    "    :param train_batch_size: Tamaño del lote para el entrenamiento.\n",
    "    :param valid_batch_size: Tamaño del lote para la validación.\n",
    "    :return: Tuple de DataLoaders para entrenamiento y validación.\n",
    "    \"\"\"\n",
    "    # Crear DataLoader para el conjunto de entrenamiento\n",
    "    train_loader = DataLoader(train_dataset, batch_size=train_batch_size, shuffle=True)\n",
    "    \n",
    "    # Crear DataLoader para el conjunto de validación\n",
    "    valid_loader = DataLoader(valid_dataset, batch_size=valid_batch_size, shuffle=False)\n",
    "    \n",
    "    return train_loader, valid_loader\n",
    "\n",
    "def prepare_dataloaders(kfold_datasets, tokenizer, max_len, train_batch_size, valid_batch_size):\n",
    "    \"\"\"\n",
    "    Esta función asume que kfold_datasets contiene pares de DataFrames de pandas para entrenamiento y validación en cada fold.\n",
    "    \"\"\"\n",
    "    train_dataloaders = []\n",
    "    valid_dataloaders = []\n",
    "    \n",
    "    for fold in kfold_datasets:\n",
    "        # Asumiendo que cada fold es un diccionario con 'train' y 'validation' como DataFrames de pandas\n",
    "        train_dataset, valid_dataset = fold['train'], fold['validation']\n",
    "        \n",
    "        # Inicializar NAICSDataset para cada DataFrame\n",
    "        train_data = NAICSDataset(dataframe=train_dataset, tokenizer=tokenizer, max_len=max_len)\n",
    "        valid_data = NAICSDataset(dataframe=valid_dataset, tokenizer=tokenizer, max_len=max_len)\n",
    "        \n",
    "        # Crear DataLoaders\n",
    "        train_loader, valid_loader = create_data_loaders_from_datasets(train_data, valid_data, tokenizer, max_len, train_batch_size, valid_batch_size)\n",
    "        \n",
    "        # Agregar a las listas\n",
    "        train_dataloaders.append(train_loader)\n",
    "        valid_dataloaders.append(valid_loader)\n",
    "    \n",
    "    return train_dataloaders, valid_dataloaders\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DistilBertModel\n",
    "import torch\n",
    "\n",
    "class DistilBERTClass(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DistilBERTClass, self).__init__()\n",
    "        # Cargar DistilBERT preentrenado\n",
    "        self.l1 = DistilBertModel.from_pretrained(\"distilbert-base-uncased\")\n",
    "        # Una capa lineal que reduce la dimensionalidad de la salida de DistilBERT de 768 a 768.\n",
    "        # Esto actúa como una capa \"pre-clasificadora\", pero en este caso, no cambia la dimensión.\n",
    "        self.pre_classifier = torch.nn.Linear(768, 768)\n",
    "        # Capa de dropout para regularización, ayudando a prevenir el sobreajuste\n",
    "        self.dropout = torch.nn.Dropout(0.3)  # Aumento la tasa de dropout para una mayor regularización\n",
    "        # Capa clasificadora final que mapea de 768 características a 24 categorías (tus etiquetas)\n",
    "        self.classifier = torch.nn.Linear(768, 24)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        \"\"\"\n",
    "        Forward pass del modelo.\n",
    "        \n",
    "        :param input_ids: IDs de tokens para la entrada.\n",
    "        :param attention_mask: Máscara de atención para evitar que el modelo atienda a tokens de padding.\n",
    "        \"\"\"\n",
    "        # Obtener la salida de DistilBERT. No necesita token_type_ids porque DistilBERT no los usa.\n",
    "        output_1 = self.l1(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        hidden_state = output_1[0]  # La salida completa de la secuencia.\n",
    "        pooler = hidden_state[:, 0]  # Tomamos solo la primera posición de todos los embeddings (representando [CLS])\n",
    "        pooler = self.pre_classifier(pooler)  # Pasamos por la capa pre-clasificadora\n",
    "        pooler = torch.nn.Tanh()(pooler)  # Aplicamos la función de activación Tanh\n",
    "        pooler = self.dropout(pooler)  # Aplicamos dropout para regularización\n",
    "        output = self.classifier(pooler)  # La salida de la capa clasificadora final\n",
    "        return output\n",
    "\n",
    "# Inicialización del modelo\n",
    "model = DistilBERTClass()\n",
    "model.to(device)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "zrive-ds-J_lydEot-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
